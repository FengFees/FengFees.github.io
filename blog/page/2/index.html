<!DOCTYPE html>
<html lang="zh-CN">

  <head>
  <meta charset="utf-8">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Blogs</title>
  <meta name="author" content="浙大SEL实验室" />
  
  
  
  
  
  <meta name="description" content="">

  <meta name="generator" content="Hugo 0.74.3" />

  
  <link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>

  
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.11.2/css/all.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  
  <link href="/css/animate.css" rel="stylesheet">

  
  
    <link href="/css/style.default.css" rel="stylesheet" id="theme-stylesheet">
  

  
  <link href="/css/custom.css" rel="stylesheet">

  
  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png" />

  
  <link href="/css/owl.carousel.css" rel="stylesheet">
  <link href="/css/owl.theme.css" rel="stylesheet">

  
  <link rel="alternate" href="https://fengfees.github.io/index.xml" type="application/rss+xml" title="浙大SEL实验室">

  
  
  
  
  
  
  <meta property="og:updated_time" content="2016-11-30T17:41:43Z">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Blogs">
  
  <meta name="twitter:description" content="">
  

</head>


  <body>

    <div id="all">

        <header class="navbar-affixed-top" data-spy="affix" data-offset-top="62">
    <div class="navbar navbar-default yamm" role="navigation" id="navbar">    
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="/">
                    <img src="/img/logo.png" alt="Blogs logo" class="hidden-xs hidden-sm">
                    <img src="/" alt="Blogs logo" class="visible-xs visible-sm">
                    <span class="sr-only">Blogs - </span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only"></span>
                        <i class="fas fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  
                  
                  
                  <li class="dropdown">
                    
                    <a href="/">主页</a>
                    
                  </li>
                  
                  
                  <li class="dropdown active">
                    
                    <a href="/blog/">博客</a>
                    
                  </li>
                  
                  
                  <li class="dropdown">
                    
                    <a href="/community/">社区</a>
                    
                  </li>
                  
                  
                  <li class="dropdown">
                    
                    <a href="/archives/">归档</a>
                    
                  </li>
                  
                  
                  <li class="dropdown">
                    
                    <a href="/contact/">关于</a>
                    
                  </li>
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">    
                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">
                    <button type="submit" class="btn btn-template-main"><i class="fas fa-search"></i></button>
                </span>
                    </div>
                </form>
            </div>
            
        </div>
    </div>
</header>




        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>Blogs</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            <div class="container">
                <div class="row">
                    

                    <div class="col-md-9" id="blog-listing-medium">

                        
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83docker-container%E7%BD%91%E7%BB%9C-%E4%B8%8A/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83docker-container%E7%BD%91%E7%BB%9C-%E4%B8%8A/">Docker源码分析（七）：Docker Container网络 （上）</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83docker-container%E7%BD%91%E7%BB%9C-%E4%B8%8A/"><i class="far fa-calendar"></i> 2015-01-26</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">本文开始介绍docker container的网络模型。
1. 前言(什么是Docker Container) 如今，Docker技术大行其道，大家在尝试以及玩转Docker的同时，肯定离不开一个概念，那就是“容器”或者“Docker Container”。那么我们首先从实现的角度来看看“容器”或者“Docker Container”到底为何物。 逐渐熟悉Docker之后，大家肯定会深深得感受到：应用程序在Docker Container内部的部署与运行非常便捷，只要有Dockerfile，应用一键式的部署运行绝对不是天方夜谭； Docker Container内运行的应用程序可以受到资源的控制与隔离，大大满足云计算时代应用的要求。毋庸置疑，Docker的这些特性，传统模式下应用是完全不具备的。然而，这些令人眼前一亮的特性背后，到底是谁在“作祟”，到底是谁可以支撑Docker的这些特性？不知道这个时候，大家是否会联想到强大的Linux内核。 其实，这很大一部分功能都需要归功于Linux内核。那我们就从Linux内核的角度来看看Docker到底为何物，先从Docker Container入手。关于Docker Container，体验过的开发者第一感觉肯定有两点：内部可以跑应用（进程），以及提供隔离的环境。当然，后者肯定也是工业界称之为“容器”的原因之一。 既然Docker Container内部可以运行进程，那么我们先来看Docker Container与进程的关系，或者容器与进程的关系。首先，我提出这样一个问题供大家思考“容器是否可以脱离进程而存在”。换句话说，能否创建一个容器，而这个容器内部没有任何进程。 可以说答案是否定的。既然答案是否定的，那说明不可能先有容器，然后再有进程，那么问题又来了，“容器和进程是一起诞生，还是先有进程再有容器呢？”可以说答案是后者。以下将慢慢阐述其中的原因。 阐述问题“容器是否可以脱离进程而存在”的原因前，相信大家对于以下的一段话不会持有异议：通过Docker创建出的一个Docker Container是一个容器，而这个容器提供了进程组隔离的运行环境。那么问题在于，容器到底是通过何种途径来实现进程组运行环境的“隔离”。这时，就轮到Linux内核技术隆重登场了。 说到运行环境的“隔离”，相信大家肯定对Linux的内核特性namespace和cgroup不会陌生。namespace主要负责命名空间的隔离，而cgroup主要负责资源使用的限制。其实，正是这两个神奇的内核特性联合使用，才保证了Docker Container的“隔离”。那么，namespace和cgroup又和进程有什么关系呢？问题的答案可以用以下的次序来说明：
 父进程通过fork创建子进程时，使用namespace技术，实现子进程与其他进程（包含父进程）的命名空间隔离； 子进程创建完毕之后，使用cgroup技术来处理子进程，实现进程的资源使用限制； 系统在子进程所处namespace内部，创建需要的隔离环境，如隔离的网络栈等； namespace和cgroup两种技术都用上之后，进程所处的“隔离”环境才真正建立，这时“容器”才真正诞生！  从Linux内核的角度分析容器的诞生，精简的流程即如以上4步，而这4个步骤也恰好巧妙的阐述了namespace和cgroup这两种技术和进程的关系，以及进程与容器的关系。进程与容器的关系，自然是：容器不能脱离进程而存在，先有进程，后有容器。然而，大家往往会说到“使用Docker创建Docker Container（容器），然后在容器内部运行进程”。对此，从通俗易懂的角度来讲，这完全可以理解，因为“容器”一词的存在，本身就较为抽象。如果需要更为准确的表述，那么可以是：“使用Docker创建一个进程，为这个进程创建隔离的环境，这样的环境可以称为Docker Container（容器），然后再在容器内部运行用户应用进程。”当然，笔者的本意不是想否定很多人对于Docker Container或者容器的认识，而是希望和读者一起探讨Docker Container底层技术实现的原理。 对于Docker Container或者容器有了更加具体的认识之后，相信大家的眼球肯定会很快定位到namespace和cgroup这两种技术。Linux内核的这两种技术，竟然能起到如此重大的作用，不禁为之赞叹。那么下面我们就从Docker Container实现流程的角度简要介绍这两者。 首先讲述一下namespace在容器创建时的用法，首先从用户创建并启动容器开始。当用户创建并启动容器时，Docker Daemon 会fork出容器中的第一个进程A（暂且称为进程A，也就是Docker Daemon的子进程）。Docker Daemon执行fork时，在clone系统调用阶段会传入5个参数标志CLONE_NEWNS、CLONE_NEWUTS、CLONE_NEWIPC、CLONE_NEWPID和CLONE_NEWNET（目前Docker 1.2.0还没有完全支持user namespace）。Clone系统调用一旦传入了这些参数标志，子进程将不再与父进程共享相同的命名空间（namespace），而是由Linux为其创建新的命名空间（namespace），从而保证子进程与父进程使用隔离的环境。另外，如果子进程A再次fork出子进程B和C，而fork时没有传入相应的namespace参数标志，那么此时子进程B和C将会与A共享同一个命令空间（namespace）。如果Docker Daemon再次创建一个Docker Container，容器内第一个进程为D，而D又fork出子进程E和F，那么这三个进程也会处于另外一个新的namespace。两个容器的namespace均与Docker Daemon所在的namespace不同。Docker关于namespace的简易示意图如下： 
图1.1 Docker中namespace示意图
再说起cgroup，大家都知道可以使用cgroup为进程组做资源的控制。与namespace不同的是，cgroup的使用并不是在创建容器内进程时完成的，而是在创建容器内进程之后再使用cgroup，使得容器进程处于资源控制的状态。换言之，cgroup的运用必须要等到容器内第一个进程被真正创建出来之后才能实现。当容器内进程被创建完毕，Docker Daemon可以获知容器内进程的PID信息，随后将该PID放置在cgroup文件系统的指定位置，做相应的资源限制。 可以说Linux内核的namespace和cgroup技术，实现了资源的隔离与限制。那么对于这种隔离与受限的环境，是否还需要配置其他必需的资源呢。这回答案是肯定的，网络栈资源就是在此时为容器添加。当为容器进程创建完隔离的运行环境时，发现容器虽然已经处于一个隔离的网络环境（即新的network namespace），但是进程并没有独立的网络栈可以使用，如独立的网络接口设备等。此时，Docker Daemon会将Docker Container所需要的资源一一为其配备齐全。网络方面，则需要按照用户指定的网络模式，配置Docker Container相应的网络资源。
2. Docker Container网络分析内容安排 Docker Container网络篇将从源码的角度，分析Docker Container从无到有的过程中，Docker Container网络创建的来龙去脉。Docker Container网络创建流程可以简化如下图： 
图2.1 Docker Container网络创建流程图
Docker Container网络篇分析的主要内容有以下5部分：</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83docker-container%E7%BD%91%E7%BB%9C-%E4%B8%8A/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/docker%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8Apipework%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E8%B7%B5/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/docker%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8Apipework%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E8%B7%B5/">Docker网络详解及pipework源码解读与实践</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/docker%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8Apipework%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E8%B7%B5/"><i class="far fa-calendar"></i> 2015-01-16</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">Docker作为目前最火的轻量级容器技术，有很多令人称道的功能，如Docker的镜像管理。然而，Docker同样有着很多不完善的地方，网络方面就是Docker比较薄弱的部分。因此，我们有必要深入了解Docker的网络知识，以满足更高的网络需求。本文首先介绍了Docker自身的4种网络工作方式，然后通过3个样例 —— 将Docker容器配置到本地网络环境中、单主机Docker容器的VLAN划分、多主机Docker容器的VLAN划分，演示了如何使用pipework帮助我们进行复杂的网络设置，以及pipework是如何工作的。
1. Docker的4种网络模式 我们在使用docker run创建Docker容器时，可以用--net选项指定容器的网络模式，Docker有以下4种网络模式：
 host模式，使用--net=host指定。 container模式，使用--net=container:NAME_or_ID指定。 none模式，使用--net=none指定。 bridge模式，使用--net=bridge指定，默认设置。  下面分别介绍一下Docker的各个网络模式。
1.1 host模式 众所周知，Docker使用了Linux的Namespaces技术来进行资源隔离，如PID Namespace隔离进程，Mount Namespace隔离文件系统，Network Namespace隔离网络等。一个Network Namespace提供了一份独立的网络环境，包括网卡、路由、Iptable规则等都与其他的Network Namespace隔离。一个Docker容器一般会分配一个独立的Network Namespace。但如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。 例如，我们在10.10.101.105/24的机器上用host模式启动一个含有web应用的Docker容器，监听tcp80端口。当我们在容器中执行任何类似ifconfig命令查看网络环境时，看到的都是宿主机上的信息。而外界访问容器中的应用，则直接使用10.10.101.105:80即可，不用任何NAT转换，就如直接跑在宿主机中一样。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。
1.2 container模式 在理解了host模式后，这个模式也就好理解了。这个模式指定新创建的容器和已经存在的一个容器共享一个Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo网卡设备通信。
1.3 none模式 这个模式和前两个不同。在这种模式下，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。
1.4 bridge模式 bridge模式是Docker默认的网络设置，此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上。下面着重介绍一下此模式。
1.4.1 bridge模式的拓扑
当Docker server启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。接下来就要为容器分配IP了，Docker会从RFC1918所定义的私有IP网段中，选择一个和宿主机不同的IP地址和子网分配给docker0，连接到docker0的容器就从这个子网中选择一个未占用的IP使用。如一般Docker会使用172.17.0.0/16这个网段，并将172.17.42.1/16分配给docker0网桥（在主机上使用ifconfig命令是可以看到docker0的，可以认为它是网桥的管理接口，在宿主机上作为一块虚拟网卡使用）。单机环境下的网络拓扑如下，主机地址为10.10.101.105/24。  Docker完成以上网络配置的过程大致是这样的：
 在主机上创建一对虚拟网卡veth pair设备。veth设备总是成对出现的，它们组成了一个数据的通道，数据从一个设备进入，就会从另一个设备出来。因此，veth设备常用来连接两个网络设备。 Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0。另一端放在主机中，以veth65f9这样类似的名字命名，并将这个网络设备加入到docker0网桥中，可以通过brctl show命令查看。  从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。  网络拓扑介绍完后，接着介绍一下bridge模式下容器是如何通信的。
1.4.2 bridge模式下容器的通信
在bridge模式下，连在同一网桥上的容器可以相互通信（若出于安全考虑，也可以禁止它们之间通信，方法是在DOCKER_OPTS变量中设置--icc=false，这样只有使用--link才能使两个容器通信）。 容器也可以与外部通信，我们看一下主机上的Iptable规则，可以看到这么一条
\-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE  这条规则会将源地址为172.17.0.0/16的包（也就是从Docker容器产生的包），并且不是从docker0网卡发出的，进行源地址转换，转换成主机网卡的地址。这么说可能不太好理解，举一个例子说明一下。假设主机有一块网卡为eth0，IP地址为10.10.101.105/24，网关为10.10.101.254。从主机上一个IP为172.17.0.1/16的容器中ping百度（180.76.3.151）。IP包首先从容器发往自己的默认网关docker0，包到达docker0后，也就到达了主机上。然后会查询主机的路由表，发现包应该从主机的eth0发往主机的网关10.10.105.254/24。接着包会转发给eth0，并从eth0发出去（主机的ip_forward转发应该已经打开）。这时候，上面的Iptable规则就会起作用，对包做SNAT转换，将源地址换为eth0的地址。这样，在外界看来，这个包就是从10.10.101.105上发出来的，Docker容器对外是不可见的。 那么，外面的机器是如何访问Docker容器的服务呢？我们首先用下面命令创建一个含有web应用的容器，将容器的80端口映射到主机的80端口。</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/docker%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8Apipework%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E8%B7%B5/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%ADdocker-daemon%E7%BD%91%E7%BB%9C/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%ADdocker-daemon%E7%BD%91%E7%BB%9C/">Docker源码分析（六）：Docker Daemon网络</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%ADdocker-daemon%E7%BD%91%E7%BB%9C/"><i class="far fa-calendar"></i> 2015-01-05</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">本文介绍docker daemon的网络模型。
摘要: Docker的容器特性和镜像特性已然为Docker实践者带来了诸多效益，然而Docker的网络特性却不能让用户满意。本文从Docker的网络模式入手，分析了Docker Daemon创建网络环境的详细流程，其中着重于分析Docker桥接模式的创建，为之后Docker Container创建网络环境做铺垫。
前言 Docker作为一个开源的轻量级虚拟化容器引擎技术，已然给云计算领域带来了新的发展模式。Docker借助容器技术彻底释放了轻量级虚拟化技术的威力，让容器的伸缩、应用的运行都变得前所未有的方便与高效。同时，Docker借助强大的镜像技术，让应用的分发、部署与管理变得史无前例的便捷。然而，Docker毕竟是一项较为新颖的技术，在Docker的世界中，用户并非一劳永逸，其中最为典型的便是Docker的网络问题。 毋庸置疑，对于Docker管理者和开发者而言，如何有效、高效的管理Docker容器之间的交互以及Docker容器的网络一直是一个巨大的挑战。目前，云计算领域中，绝大多数系统都采取分布式技术来设计并实现。然而，在原生态的Docker世界中，Docker的网络却是不具备跨宿主机能力的，这也或多或少滞后了Docker在云计算领域的高速发展。 工业界中，Docker的网络问题的解决势在必行，在此环境下，很多IT企业都开发了各自的新产品来帮助完善Docker的网络。这些企业中不乏像Google一样的互联网翘楚企业，同时也有不少初创企业率先出击，在最前沿不懈探索。这些新产品中有，Google推出的容器管理和编排开源项目Kubernetes，Zett.io公司开发的通过虚拟网络连接跨宿主机容器的工具Weave，CoreOS团队针对Kubernetes设计的网络覆盖工具Flannel，Docker官方的工程师Jérôme Petazzoni自己设计的SDN网络解决方案Pipework，以及SocketPlane项目等。 对于Docker管理者与开发者而言，Docker的跨宿主机通信能力固然重要，但Docker自身的网络架构也同样重要。只有深入了解Docker自身的网络设计与实现，才能在这基础上扩展Docker的跨宿主机能力。 Docker自身的网络主要包含两部分：Docker Daemon的网络配置，Docker Container的网络配置。本文主要分析Docker Daemon的网络。
Docker Daemon网络分析内容安排 本文从源码的角度，分析Docker Daemon在启动过程中，为Docker配置的网络环境，章节安排如下：
 Docker Daemon网络配置； 运行Docker Daemon网络初始化任务； 创建Docker网桥。  本文为《Docker源码分析系列》第六篇——Docker Daemon网络篇，第七篇将安排Docker Container网络篇。
Docker Daemon网络配置 Docker环境中，Docker管理员完全有权限配置Docker Daemon运行过程中的网络模式。 关于Docker的网络模式，大家最熟知的应该就是“桥接”的模式。下图为桥接模式下，Docker的网络环境拓扑图（包括Docker Daemon网络环境和Docker Container网络环境）： 
图3.1 Docker网络桥接示意图
然而，“桥接”是Docker网络模式中最为常用的模式。除此之外，Docker还为用户提供了更多的可选项，下文将对此一一说来。
Docker Daemon网络配置接口 Docker Daemon每次启动的过程中，都会初始化自身的网络环境，这样的网络环境最终为Docker Container提供网络通信服务。 Docker管理员配置Docker的网络环境，可以在Docker Daemon启动时，通过Docker提供的接口来完成。换言之，可以使用docker二进制可执行文件，运行docker -d并添加相应的flag参数来完成。 其中涉及的flag参数有EnableIptables、EnableIpForward、BridgeIface、BridgeIP以及InterContainerCommunication。该五个参数的定义位于./docker/daemon/config.go，具体代码如下：
flag.BoolVar(&amp;config.EnableIptables, []string{&quot;#iptables&rdquo;, &ldquo;-iptables&rdquo;}, true, &ldquo;Enable Docker&rsquo;s addition of iptables rules&rdquo;) flag.BoolVar(&amp;config.EnableIpForward, []string{&quot;#ip-forward&rdquo;, &ldquo;-ip-forward&rdquo;}, true, &ldquo;Enable net.ipv4.ip_forward&rdquo;) flag.StringVar(&amp;config.BridgeIP, []string{&quot;#bip&rdquo;, &ldquo;-bip&rdquo;}, &ldquo;&rdquo;, &ldquo;Use this CIDR notation address for the network bridge&rsquo;s IP, not compatible with -b&rdquo;) flag.</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%ADdocker-daemon%E7%BD%91%E7%BB%9C/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bvolumes/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bvolumes/">Google Kubernetes设计文档之Volumes</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bvolumes/"><i class="far fa-calendar"></i> 2015-01-02</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">摘要：
Kubernetes是Google开源的容器集群管理系统，构建于Docker之上，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等功能。本文描述了Kubernetes中Volumes的使用情况，Volume是一个能够被容器访问的目录。
Volumes 本文描述了Kubernetes中Volumes的使用情况，建议在阅读本文前，首先熟悉pods。 Volume是一个能够被容器访问的目录，它可能还会包含一些数据。Kubernetes Volumes与Docker Volumes类似，但并不完全相同。 一个Pod会在它的ContainerManifest 属性中指明其容器需要哪些Volumes。 容器中的进程可见的文件系统视图由两个源组成：一个单独的Docker image和零个或多个Volumes。Docker image位于文件层次结构的根部。所有的Volumes都挂载在Docker image的节点上。Volumes不能挂载在其他的Volumes上，也没有连接其他Volumes的硬链接。Pod中的每个容器都单独地指明了其image挂载的Volume。这会通过VolumeMount属性来确定。
资源 Volume的存储介质(如硬盘、固态硬盘或内存)是由保存kubelet根目录(一般为/var/lib/kubelet)的文件系统的存储介质决定的。一个EmptyDir或者PersistentDir类型的Volume可以使用多少空间是没有限制的，同时在容器或者pods间也不存在隔离。 将来，我们预计一个Volume将能够通过使用资源规范来请求一个确定大小的空间；同时，对于包含多种存储介质的集群，将可以选择Volume使用的介质类型。
Volumes的类型 Kubernetes现在支持三种类型的Volumes，但将来会支持更多的类型。
EmptyDir 一个EmptyDir Volume是在Pod绑定到Node时创建的。当第一条容器命令启动时，它的初始状态为空。在同一个Pod上的容器可以读写EmptyDir中的相同文件。当一个Pod被解绑，在EmptyDir中的数据将永久性删除。 EmptyDir的一些用途如下：
 暂存空间，例如用于基于磁盘的归并排序或者长计算的检查点； 一个目录，由一个内容管理容器填充数据，同时由一个网络服务器容器供应数据。  目前，用户无法控制EmptyDir使用的介质种类。如果Kubelet的配置是使用硬盘，那么所有的EmptyDirectories都将创建在该硬盘上。将来，可以预料的是Pods将可以控制EmptyDir是位于硬盘、固态硬盘还是基于内存的tmpfs上。
HostDir 一个HostDir的Volume将可以访问当前宿主机节点上的文件。 HostDir的一些用途如下：
 运行一个需要访问Docker内部结构的容器；可以访问/var/lib/docker这个HostDir ；在容器中运行cAdvisor；可以访问/dev/cgroups这个HostDir。  当使用该类型的Volume时，需要格外小心，因为：
 具有相同配置的pods(例如由同一个podTemplate创建的pods)可能在不同宿主节点上由于宿主机上的目录和文件不同而有着不同的访问结果； 当Kubernetes增加资源敏感调度，按其计划，它将不能考虑到HostDir使用的资源。  GCEPersistentDisk 重要提示：必须创建并格式化一个永久磁盘(PD)才能使用GCEPersistentDisk。 拥有GCEPersistentDisk的Volume可以访问谷歌计算引擎（Google Compute Engine, GCE）的永久磁盘上的文件。 使用GCEPersistentDisk时，有一些限制条件：
 节点(运行kubelet的节点)需要是GCE虚拟机； 这些虚拟机需要在相同的GCE项目中，同时被划作PD； 避免使用相同Volume来创建多个pods  如果多个pods引用相同的Volume，并且都部署在同一台机器上，不论它们是只读还是可读写的，那么第二个pod的部署都将失败； 只有使用只读加载的文件系统的pod才能创建复制控制器。    创建一个PD 在你能够在pod上使用GCE PD前，你需要先创建并格式化它。 我们正在积极努力得使这个过程更加精简容易。
DISK\_NAME=my-data-disk DISK\_SIZE=500GB ZONE=us-central1-a gcloud compute disks create --size=$DISK\_SIZE --zone=$ZONE $DISK\_NAME gcloud compute instances attach-disk --zone=$ZONE --disk=$DISK\_NAME --device-name temp-data kubernetes-master gcloud compute ssh --zone=$ZONE kubernetes-master \\ --command &quot;sudo mkdir /mnt/tmp &amp;&amp; sudo /usr/share/google/safe\_format\_and\_mount /dev/disk/by-id/google-temp-data /mnt/tmp&quot; gcloud compute instances detach-disk --zone=$ZONE --disk $DISK\_NAME kubernetes-master  GCE PD的配置实例：</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bvolumes/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87/">Google Kubernetes设计文档之网络篇</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87/"><i class="far fa-calendar"></i> 2014-12-29</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">摘要： Kubernetes是Google开源的容器集群管理系统，构建于Docker之上，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等功能。其从Docker默认网络模型中独立出来形成了一套自己的网络模型，本文将详细介绍。
模型和动机 Kubernetes从Docker默认的网络模型中独立出来形成一套自己的网络模型。该网络模型的目标是：每一个pod都拥有一个扁平化共享网络命名空间的IP，通过该IP，pod就能够跨网络与其它物理机和容器进行通信。一个pod一个IP模型创建了一个干净、反向兼容的模型，在该模型中，从端口分配、网络、域名解析、服务发现、负载均衡、应用配置和迁移等角度，pod都能够被看成虚拟机或物理机。 另一方面，动态端口分配需要以下方面的支持： 1，固定端口（例如：用于外部可访问服务）和动态分配端口； 2，分割集中分配和本地获取的动态端口； 不过，这不但使调度复杂化（因为端口是一种稀缺资源），而且应用程序的配置也将变得复杂，具体表现为端口冲突、重用和耗尽。 3，使用非标准方法进行域名解析（例如：etcd而不是DNS）； 4，对使用标准域名/地址解析机制的程序（例如：web浏览器）使用代理和/或重定向。 5，除了监控和缓存实例的非法地址/端口变化外，还要监控用户组成员变化以及阻止容器/pod迁移（例如：使用CRIU）。 NAT将地址空间分段的做法引入了额外的复杂性，这将带来诸如破坏自注册机制等问题。 在一个pod一个IP模型中，从网络角度看，在一个pod中的所有用户容器都像是在同一台宿主机中那样。它们能够在本地访问其它用户容器的端口。暴露给主机网卡的端口是通过普通Docker方式实现的。所有pod中的容器能够通过他们“10”网段（10.x.x.x）的IP地址进行通信。
除了能够避免上述动态端口分配带来的问题，该方案还能使应用平滑地从非容器环境（物理机或虚拟机）迁移到同一个pod内的容器环境。在同一台宿主机上运行应用程序栈这种场景已经找到避免端口冲突的方法（例如：通过配置环境变量）并使客户端能够找到这些端口。 该方案确实降低了pod中容器之间的隔离性&ndash;尽管端口可能存在冲突而且也不存在pod内跨容器的私有端口，但是对于需要自己的端口范围的应用程序可以运行在不同的pod中，而对于需要进行私有通信的进程则可以运行在同一个容器内。另外，该方案假定的前提条件是：在同一个pod中的容器共享一些资源（例如：磁盘卷、处理器、内存等），因此损失部分隔离性也在可接受范围之内。此外，尽管用户能够指定不同容器归属到同一个pod，但一般情况下不能指定不同pod归属于同一台主机。 当任意一个容器调用SIOCGIFADDR（发起一个获取网卡IP地址的请求）时，它所获得的IP和与之通信的容器看到的IP是一样的&ndash;每个pod都有一个能够被其它pod识别的IP。通过无差别地对待容器和pdo内外部的IP和端口，我们创建了一个非NAT的扁平化地址空间。&ldquo;ip addr show&quot;能够像预期那样正常工作。该方案能够使所有现有的域名解析/服务发现机制：包括自注册机制和分配IP地址的应用程序在容器外能够正常运行（我们应该用etcd、Euraka（用于Acme Air）或Consul等软件测试该方案）。对pod之间的网络通信，我们应该持乐观态度。在同一个pod中的容器之间更倾向于通过内存卷（例如：tmpfs）或IPC（进程间通信）的方式进行通信。 该模型与标准Docker模型不同。在该模型中，每个容器会得到一个“172”网段（172.x.x.x）的IP地址，而且通过SIOCGIFADDR也只能看到一个“172”网段（172.x.x.x）的IP地址。如果这些容器与其它容器连接，对方容器看到的IP地址与该容器自己通过SIOCGIFADDR请求获取的IP地址不同。简单地说，你永远无法在容器中注册任何东西或服务，因为一个容器不可能通过其私有IP地址被外界访问到。 我们想到的一个解决方案是增加额外的地址层：以pod为中心的一个容器一个IP模式。每个容器只拥有一个本地IP地址，且只在pod内可见。这将使得容器化应用程序能够更加容易地从物理/虚拟机迁移到pod，但实现起来很复杂（例如：要求为每个pod创建网桥，水平分割/虚拟私有的 DNS），而且可以预见的是，由于新增了额外的地址转换层，将破坏现有的自注册和IP分配机制。
当前实现 Google计算引擎（GCE）集群配置了高级路由，使得每个虚拟机都有额外的256个可路由IP地址。这些是除了分配给虚拟机的通过NAT用于访问互联网的“主”IP之外的IP。该实现在Docker外部创建了一个叫做cbr0的网桥（为了与docker0网桥区别开），该网桥只负责对从容器内部流向外部的网络流量进行NAT转发。 目前，从“主”IP（即互联网，如果制定了正确的防火墙规则的话）发到映射端口的流量由Docker的用户模式进行代理转发。未来，端口转发应该由Kubelet或Docker通过iptables进行：Issue #15。 启动Docker时附加参数：DOCKER_OPTS=&rdquo;&ndash;bridge cbr0 &ndash;iptables=false&rdquo;。 并用SaltStack在每个node中创建一个网桥，代码见container_bridge.py：
cbr0: container\_bridge.ensure: - cidr: {{ grains\['cbr-cidr'\] }} ... grains: roles: - kubernetes-pool cbr-cidr: $MINION\_IP\_RANGE\`  在GCE中，我们让以下IP地址可路由：
\`gcloud compute routes add &quot;${MINION\_NAMES\[$i\]}&quot; \\ --project &quot;${PROJECT}&quot; \\ --destination-range &quot;${MINION\_IP\_RANGES\[$i\]}&quot; \\ --network &quot;${NETWORK}&quot; \\ --next-hop-instance &quot;${MINION\_NAMES\[$i\]}&quot; \\ --next-hop-instance-zone &quot;${ZONE}&quot; &amp;  以上代码中的MINION_IP_RANGES是以10.开头的24位网络号IP地址空间（10.x.x.x/24）。 尽管如此，GCE本身并不知道这些IP地址的任何信息。 这些IP地址不是外部可路由的，因此，那些有与外界通信需求的容器需要使用宿主机的网络。如果外部流量要转发给虚拟机，它只会被转发给虚拟机的主IP（该IP不会被分配给任何一个pod），因此我们使用docker的-p标记把暴露的端口映射到主网卡上。该方案的带来的副作用是不允许两个不同的pod暴露同一个端口（更多相关讨论见Issue #390）。 我们创建了一个容器用于管理pod的网络命名空间，该网络容器内有一个本地回环设备和一块虚拟以太网卡。所有的用户容器从该pod的网络容器那里获取他们的网络命名空间。 Docker在它的“container”网络模式下从网桥（我们为每个节点上创建了一个网桥）那里分配IP地址，具体步骤如下： 1，使用最小镜像创建一个普通容器（从网络角度）并运行一条永远阻塞的命令。这不是一个用户定义的容器，给它一个特别的众所周知的名字。</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E6%9C%8D%E5%8A%A1%E7%AF%87/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E6%9C%8D%E5%8A%A1%E7%AF%87/">Google Kubernetes设计文档之服务篇</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E6%9C%8D%E5%8A%A1%E7%AF%87/"><i class="far fa-calendar"></i> 2014-12-23</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">摘要：
Kubernetes是Google开源的容器集群管理系统，构建于Docker之上，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等功能。 Pod是创建、调度和管理的最小部署单位，本文详细介绍这些Pod之间的通信和调度。
概述 Kubernetes中 Pods 不是一成不变的。它们可以随着时间进行迁移，特别是当受到 ReplicationControllers支配时。虽然每个pod都有属于自己的IP地址，但是却不能保证每个Pod的IP地址随着时间的变化依然保持不变。这就导致了一个问题：如果在Kubernetes集群里，有一系列的pods（我们姑且称之为后端）为其他的pods（称为前端）提供功能，那前端该如何去找到后端？
服务 Kubernetes中的服务是一种抽象概念，它定义了一个pods逻辑集合以及访问它们的策略，有时它也被称为微服务（Micro-service）。服务的目标是提供一种桥梁，使得非Kubernetes原生应用程序，在无需为Kubernetes编写特定代码的前提下，轻松访问后端。服务会为用户提供一对IP地址和port端口，用于在访问时重定向到相应的后端。服务里Pods集合的选定是由一个标签选择器（label selector）来完成的。 举个例子，首先假设一个“镜像处理”后端，它运行着三个可用的副本。这些副本是无状态的，前端根本不关心自己具体使用的是后端的哪个副本。因此，尽管组成后端集合的实际pods可能已经发生了改变，但是前端用户完全不需要知晓这些改变。这种服务的抽象性实现了前端访问与后端服务的解耦。
定义服务 这里是一个使用服务的例子。在Kubernetes中，服务是REST对象，类似于pod。如pod一般，服务的定义，可以通过一个发给apiserver的POST请求，来完成创建一个新的实例。例如，假设你有一组pods，都暴露9376端口，并携带一个&quot;app=MyApp&quot;的标签。
{ &ldquo;id&rdquo;: &ldquo;myapp&rdquo;, &ldquo;selector&rdquo;: { &ldquo;app&rdquo;: &ldquo;MyApp&rdquo; }, &ldquo;containerPort&rdquo;: 9376, &ldquo;protocol&rdquo;: &ldquo;TCP&rdquo;, &ldquo;port&rdquo;: 8765 }
上述定义将创建一个名为&quot;myapp&quot;的新服务，它使得所有带有&quot;app=MyApp&quot;标签的pod都监听TCP协议上的端口9376。而客户可以通过端口$MYAPP_SERVICE_PORT连接到$MYAPP_SERVICE_HOST，从而访问该服务。
服务是如何工作的？ 在Kubernetes集群中的每个节点（node）上都运行着一个服务代理（service proxy）。该代理应用监听Kubernetes Master，以此来添加和删除服务对象及端点（endpoints，即满足服务标签选择器的pods），同时该代理应用还存储一个服务到端点列表的映射。它为每个服务在本地节点上打开一个端口，并转发该端口上的所有流量到后端。名义上是依据策略来执行的，但现在唯一支持的策略是轮转调度（round-robin）。 当一个pod被编入，Master为每一个存活的服务增加一组环境变量。我们支持Docker-links-compatible变量（参见makeLinkVariables）以及更简单的{SVCNAME}_SERVICE_HOST和{SVCNAME}_SERVICE_PORT变量，其中的服务名要求大写，破折号将转换为下划线。具体的服务工作图如图1 。例如，服务&quot;redis-master&quot;监听TCP端口637，并分配IP地址10.0.0.11，将产生以下环境变量：
REDIS\_MASTER\_SERVICE\_HOST=10.0.0.11 REDIS\_MASTER\_SERVICE\_PORT=6379 REDIS\_MASTER\_PORT=tcp://10.0.0.11:6379 REDIS\_MASTER\_PORT\_6379\_TCP=tcp://10.0.0.11:6379 REDIS\_MASTER\_PORT\_6379\_TCP\_PROTO=tcp REDIS\_MASTER\_PORT\_6379\_TCP\_PORT=6379 REDIS\_MASTER\_PORT\_6379\_TCP\_ADDR=10.0.0.11  这意味着要有先后次序，即一个pod希望访问的服务必须在pod本身创建之前被创建，否则环境变量不会被加载。不过支持DNS服务后，该限制将不再存在。 服务通过它的标签选择器，可以解析到0或多个端点。在服务的生命周期内，组成该服务的pods集合可以增加、缩减，或全部失效。用户只有在当他们正在使用的后端从服务中被移除时，才会遇到问题（即使如此，已经打开的连接也会因为某些协议的缘故，继续保持）。 
图1.服务工作图
细节明细 前文的内容对于大多数只是想要使用服务的人来说应该已经足够了。然而，有很多发生在这背后的事情也值得去深入了解。
避免冲突 Kubernetes的一个主要理念是，用户不应该遭遇可能导致其操作失败的情景，尤其是用户本身并未引起错误。在此背景下，我们考虑网络端口的问题—不应该让用户选择一个可能与其他用户发生冲突的端口号。否则，将是隔离上的失败。 为了让用户能够选择他们的服务端口号，我们必须确保不会引起两个服务间的冲突。我们通过为每个服务分配自己的IP地址来做到这一点。
IP和Portal 不同于路由到一个固定的pod的IP地址，服务的IP实际上并不是由单个Master响应的。相反的，我们用iptables（Linux中的数据包处理逻辑）来定义这些需要透明重定向的“虚拟”IP地址。我们将服务IP和服务端口的元组称为Portal。当用户连接到portal上，其访问会被自动转移到一个相应的端点上。实际上，服务的环境变量是依据portal的IP和端口来设定的。此外，我们将增加DNS来支撑服务的访问。 举例来说，考虑上图所展示的应用处理过程。在创建后端服务时，Kubernetes Master分配一个Portal的IP地址，例如10.0.0.1。假设服务端口是1234，portal即为是10.0.0.1:1234。Master将存储该信息，它也被集群中所有的服务代理实例所获取。当代理监测到一个新的portal，它会打开一个新的随机端口，建立一个从Portal到新端口的iptables重定向，然后开始接受对其的连接。 当用户使用portal端口连接MYAPP_SERVICE_HOST时（不论他们将其视为静态端口或视为MYAPP_SERVICE_PORT），iptables规则生效，重定向数据包到服务代理自身的端口上。服务代理选择一个后端，并开始从客户端到后端的代理通信流量。具体原理如图2。 最终的结果是，用户可以选择他们想要的任何服务端口，而没有冲突的危险。客户可以轻松连接IP和端口，而无需了解到他们正在访问哪个pods。 
图2.服务中IP和portal原理图
外部服务 对于用户应用程序的某些部分（如前端），用户希望在外部可访问的IP地址（公网IP）上暴露一个服务。 如果你希望你的服务暴露在一个公网IP地址上，你可以选择提供一个服务可以响应的&quot;publicIPs&quot;列表。这些IP地址将被绑定上服务端口，同时被映射到由服务选择的pods集合上。你随后需要负责确保到该公网IP地址的通信流量被发送到一个或多个kubernetes工作节点。与映射内部IP地址一致，每个主机上的每一条IPTables规则，将公网特定IP地址的数据包映射到内部的服务代理。 对于提供外部负载均衡设备的云服务提供商，还有一个更简单的方式来达到同样的效果。在这类供应商（如GCE）上，你可以让publicIPs为空，作为代替，你可以在服务上设置createExternalLoadBalancer标志。这会启动一个云服务提供商的特定负载均衡设备（假设它由你的云提供商支持），并用适当的值来填充这个公网IP值域。
缺点 我们预计，portals使用的iptables将在小规模上可用，而无法扩展到有着成千上万服务的大型集群。查看 portals的原始设计方案 ，可了解更多详情。
今后的工作 在将来，我们设想，代理策略可以变得比简单的轮循调度更加细致入微，比如master筛选或分片。我们还设想，一些服务将具有“真正的”负载均衡器，在这种情况下，portal将可以直接简单地传输数据包。</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E6%9C%8D%E5%8A%A1%E7%AF%87/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bpod%E7%AF%87/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bpod%E7%AF%87/">Google Kubernetes设计文档之Pod篇</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bpod%E7%AF%87/"><i class="far fa-calendar"></i> 2014-12-19</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">摘要：Kubernetes是Google开源的容器集群管理系统，构建于Docker之上，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等功能。CSDN联合浙江大学SEL实验室共同翻译其设计文档，本文为系列的第二篇：Pod。 在Kubernetes中，创建、调度和管理的最小部署单位是Pod，而不是容器。
1.什么是Pod 一个Pod对应于由若干容器组成的一个容器组，同个组内的容器共享一个存储卷(volume)。Pod主要是在容器化环境中建立了一个面向应用的“逻辑主机”模型，它可以包含一个或多个相互间紧密联系的容器。在没有容器化技术的场景里，同个Pod内的“容器”都在同一台物理或虚拟主机上运行。 Pod与容器一样都不是持续存在的，在容器的生命周期里，每个Pod被分配到节点上运行直至运行结束或被删除。当一个节点消失时，该节点上的Pod也随之被删除。每个Pod实体只会被调度一次，不会重复分配给别的节点，由replication controller负责创建新的Pod来替代旧的（在未来也可能有新的API用于Pod迁移）。
2.开发Pod的原因 2.1 资源共享和通信 Pod的存在使同个Pod下的容器之间能更方便的共享数据和通信。 同个Pod下的容器使用相同的网络命名空间、IP地址和端口区间，相互之间能通过localhost来发现和通信。在一个无层次的共享网络中，每个Pod都有一个IP地址用于跟别的物理主机和容器通信，Pod的名字就用作容器通信时的主机名。（有关网络的内容，我们稍后会翻译） 在同个Pod内运行的容器还共享一块存储卷空间，存储卷内的数据不会在容器重启后丢失，同时能被同Pod下别的容器读取。 未来的开发计划是使Pod共享IPC命名空间，CPU和内存。(参加Google的lmctfy文档，lmctfy的意思是Let me Contain That For You)
2.2 管理 相比原生的容器接口，Pod通过提供更高层次的抽象，简化了应用的部署和管理。Pods就像一个管理和横向部署/和管理的单元，主机托管、资源共享、协调复制和依赖管理都可以自动处理。
3.Pod用例 Pod能应用于构建垂直集成应用栈，但它的主要为了集中管理一些辅助程序，如：
 内容管理系统，文件和数据载入器，本地缓存管理等； - 日志和检查点备份，压缩，轮换，快照系统等； 数据变化监视，日志末端数据读取，日志和监控适配器，事件打印等； 代理，桥接和适配器； 控制器，管理器，配置编辑器和更新器。  Pod的设计并不是为了运行同一个应用的多个实例。
4.其他考虑因素 为什么不直接在单个Docker容器中运行多个程序？主要是出于以下几个原因：
 透明性：将Pod内的容器向基础设施可见，底层系统就能向容器提供如进程管理和资源监控等服务，这样能给用户带来极大便利； 解绑软件的依赖：这样单个的容器可以独立地重建和重新部署。未来有可能在Kubernetes上实现独立容器的实时更新； 易用性：用户不需要运行自己的进程管理器，也不需负责信号量和退出码的传递等； 高效性：因为底层设备负责更多了管理，容器因而能更轻量化。  为什么不直接将相近的容器集中管理呢？那种方法虽然提供了集中管理的功能，但却没有Pod所拥有的大部分便利之处，如不提供资源共享和进程间通信，无法保证容器同时开始和结束，不能简化管理等。
原文链接：Pods（编译/叶瑞浩 审校/孙宏亮、张磊） [simple-author-box]</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8Bpod%E7%AF%87/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/cf-release%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/cf-release%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/">cf-release结构解析</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/cf-release%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/"><i class="far fa-calendar"></i> 2014-12-17</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">cf-release结构解析
1. 制作时的cf-release结构解析 此处指的release统一为CloudFoundry官方给出的cf-release，不做修改。 1.1. 通过载入cf-release文件夹下config/final.yml文件，获得需要下载release文件的远程服务器网址，默认使用的提供商是s3，地址是：blob.cfblob.com  1.2. 通过config/blobs.yml，可以得到所有blobs的object_id，通过服务器地址+object_id拼接的字符串即可下载到相对应的blob内容。 1.3. 默认存储的位置为cf-release/.blobs，存储的文件名为sha1值，下载完成后会在cf-release/blobs文件夹下创建以package真实名字命名的软链接到.blobs里面各个具体的包。  1.4. 下载完所有的blobs后，开始对照cf-release/packages文件夹下各个包的spec文件逐个在blobs文件夹下找到，然后拷贝到.final_builds或者.dev_builds，根据是否加了&ndash;final参数决定。拷贝前会执行预安装脚本prepackaging，检查文件是否都存在，做一些单元测试等。执行完后把prepackaging脚本删除后压缩文件夹。 (TIPS：有时候某些不需要部署的组件，却因为过不了prepacking脚本的执行导致release做不出来，可以把prepackaging脚本删掉再制作，会自动跳过这个执行过程。) 1.5. 对所有cf-release/jobs进行的操作相对简单，除了拷贝到.final_builds或者.dev_builds以外，通过spec文件检查template等文件是否齐全。 1.6. 最后生成releases/cf-#{version}.yml文件,在dev_releases文件夹下生成cf-{version}.dev.yml release就算初步制作完成了。
2. 部署时的cf-release结构解析  2.1. 获得cf-release的配置文件： 扫描./releases以及./dev_releases文件夹，对其中的release配置文件进行排序，排序规则为数字大的优先，相同大小的数字以小数点后大的优先，两个数字都相同取没有dev标记的。 194 &gt; 193 194.1 &gt; 194 194.1 &gt; 194.1-dev 这里得到的最新的文件，就是定义当前release包所有版本的配置文件，称之为@release。 2.2. 获取部署配置文件manifest/cf.yml中，要部署的job构成的所有template。部署时定义的job在配置文件中包含多个template，每个template由多个package组成。
--- deployment: cf jobs: - name: nats template: - nats - nats_stream_forwarder - name: nfs_server template: - debian_nfs_server  2.3. 对于2.2中找出的每个template，找到其在@release文件中的version编号以及sha1值（jobs属性下），然后找到.final_builds/jobs下对应的index.yml和.dev_builds/jobs下对应的index.yml，比对两个文件中的sha1，找到对应的版本。此时我们就获得了template的全部具体信息，称之为@template。 2.4. @template下有个压缩包，后缀为.tgz，解压缩后得到job.MF文件，可获得该template的所有配置文件，配置文件需要的属性以及依赖的packages。也就是这里，我们获得了构成这个template的所有packages名字。然后我们对照之前的@release文件，又可以得到具体每个package需要的版本。 2.5. 值得注意的是，每个template由一个或多个packages构成，而每个package，由零个或多个其他packages构成，而每个package依赖哪些其它package，也在@release文件中的packages栏目下。 2.6. 通过类似的方法，我们在.final_builds和.dev_builds中的packages对应的package中可以对比出具体的package版本信息，找到需要部署的包，我们命名为@package。 至此，部署所需要的cf-release结构就已经全部解析出来了。
3. 部署 3.1. 默认的部署目录为/var/vcap，部署之前会在该目录下创建目录bosh,data,jobs,monit,packages,shared,store,sys这几个目录。 3.</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/cf-release%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/%E7%8E%A9%E8%BD%ACdocker%E9%95%9C%E5%83%8F/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/%E7%8E%A9%E8%BD%ACdocker%E9%95%9C%E5%83%8F/">玩转Docker镜像</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/%E7%8E%A9%E8%BD%ACdocker%E9%95%9C%E5%83%8F/"><i class="far fa-calendar"></i> 2014-12-16</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">**摘要：**Docker是基于Go语言开发，通过分层镜像标准化和内核虚拟化技术，使得应用开发者和运维工程师可以以统一的方式跨平台发布应用。镜像是Docker最核心的技术之一，也是应用发布的标准格式。
前言 Docker是Docker.Inc公司开源的一个基于轻量级虚拟化技术的容器引擎项目,整个项目基于Go语言开发，并遵从Apache 2.0协议。通过分层镜像标准化和内核虚拟化技术，Docker使得应用开发者和运维工程师可以以统一的方式跨平台发布应用，并且以几乎没有额外开销的情况下提供资源隔离的应用运行环境。由于众多新颖的特性以及项目本身的开放性，Docker在不到两年的时间里迅速获得诸多IT厂商的参与，其中更是包括Google、Microsoft、VMware等业界行业领导者。同时，Docker在开发者社区也是一石激起千层浪，许多如我之码农纷纷开始关注、学习和使用Docker，许多企业，尤其是互联网企业，也在不断加大对Docker的投入，大有掀起一场容器革命之势。
Docker镜像命名解析 镜像是Docker最核心的技术之一，也是应用发布的标准格式。无论你是用docker pull image，或者是在Dockerfile里面写FROM image，从Docker官方Registry下载镜像应该是Docker操作里面最频繁的动作之一了。那么在我们执行docker pull image时背后到底发生了什么呢？在回答这个问题前，我们需要先了解下docker镜像是如何命名的，这也是Docker里面比较容易令人混淆的一块概念：Registry，Repository, Tag and Image。 下面是在本地机器运行docker images的输出结果：  我们可以发现我们常说的“ubuntu”镜像其实不是一个镜像名称，而是代表了一个名为ubuntu的Repository，同时在这个Repository下面有一系列打了tag的Image，Image的标记是一个GUID，为了方便也可以通过Repository:tag来引用。 那么Registry又是什么呢？Registry存储镜像数据，并且提供拉取和上传镜像的功能。Registry中镜像是通过Repository来组织的，而每个Repository又包含了若干个Image。
 Registry包含一个或多个Repository Repository包含一个或多个Image Image用GUID表示，有一个或多个Tag与之关联  那么在哪里指定Registry呢？让我们再拉取一个更完整命名的镜像吧：  上面我试图去拉取一个ubuntu镜像，并且指定了Registry为我本机搭建的私有Registry。下面是Docker CLI中pull命令的代码片段 (docker/api/client/command.go中的CmdPull函数)  在运行时，上面的taglessRemote变量会被传入localhost:5000/ubuntu。上面代码试图从taglessRemote变量中解析出Registry的地址，在我们的例子中，它是localhost:5000。 那我们回过头再来看看下面这个耳熟能详的pull命令背后的故事吧：  我们跟着上面的示例代码，进一步进入解析函数ResolveRepositoryName的定义代码片段(docker/registry/registry.go)  我们发现，Docker CLI会判断传入的taglessRemote参数的第一部分中是否包含’.’或者&rsquo;:’，如果存在则认为第一部分是Registry地址，否则会使用Docker官方默认的Registry（即index.docker.io其实这里是一个Index Server，和Registry的区别留在后面再去深究吧），即上面代码中高亮的部分。背后的故事还没有结束，如果你向DockerHub上传过镜像，应该记得你上传的镜像名称格式为user-name/repository:tag，这样用户Bob和用户Alice可以有相同名称的Repository，通过用户名前缀作为命名空间隔离，比如Bob/ubuntu和Alice/ubuntu。官方镜像是通过用户名library来区分的，具体代码片段如下(docker/api/client/command.go中的CmdPull函数)  我们回过头再去看Docker命令行中解析Tag的逻辑吧(docker/api/client/command.go中的CmdPull函数)：  代码会试着在用户输入的Image名称中找’ : ‘后面的tag,如果不存在，会使用默认的‘DEFAULTTAG’，即‘latest’。 也就是说在我们的例子里面，命令会被解析为下面这样（注意，下面的命令不能直接运行，因为Docker CLI不允许明确指定官方Registry地址） 
配置Registry Mirror Docker之所以这么吸引人，除了它的新颖的技术外，围绕官方Registry（Docker Hub）的生态圈也是相当吸引人眼球的地方。在Docker Hub上你可以很轻松下载到大量已经容器化好的应用镜像，即拉即用。这些镜像中，有些是Docker官方维护的，更多的是众多开发者自发上传分享的。而且你还可以在Docker Hub中绑定你的代码托管系统（目前支持Github和Bitbucket）配置自动生成镜像功能，这样Docker Hub会在你代码更新时自动生成对应的Docker镜像，是不是很方便？ 不幸的是Docker Hub并没有在国内放服务器或者用国内的CDN，下载个镜像20分钟最起码，我等码农可耗不起这么长时间，老板正站在身后催着我们搬运代码呢。为了克服跨洋网络延迟，一般有两个解决方案：一是使用私有Registry，另外是使用Registry Mirror，我们下面一一展开聊聊. 方案一就是搭建或者使用现有的私有Registry，通过定期和Docker Hub同步热门的镜像，私有Registry上保存了一些镜像的副本，然后大家可以通过docker pull private-registry.com/user-name/ubuntu:latest，从这个私有Registry上拉取镜像。因为这个方案需要定期同步Docker Hub镜像，因此它比较适合于使用的镜像相对稳定，或者都是私有镜像的场景。而且用户需要显式的映射官方镜像名称到私有镜像名称，私有Registry更多被大家应用在企业内部场景。私有Registry部署也很方便，可以直接在Docker Hub上下载Registry镜像，即拉即用，具体部署可以参考官方文档。 方案二是使用Registry Mirror，它的原理类似于缓存，如果镜像在Mirror中命中则直接返回给客户端，否则从存放镜像的Registry上拉取并自动缓存在Mirror中。最酷的是，是否使用Mirror对Docker使用者来讲是透明的，也就是说在配置Mirror以后，大家可以仍然输入docker pull ubuntu来拉取Docker Hub镜像，除了速度变快了，和以前没有任何区别。 了以更便捷的方式对接Docker Hub生态圈，使用Registry Mirror自然成为我的首选。接下来我就和大家一起看看Docker使用Mirror来拉取镜像的过程。下面的例子，我使用的是由**DaoCloud**提供的Registry Mirror服务，在申请开通Mirror服务后你会得到一个Mirror地址，然后我们要做的就是把这个地址配置在Docker Server启动脚本中，重启Docker服务后Mirror配置就生效了（如何获得Mirror服务可以参考本篇文章的附录） Ubuntu下配置Docker Registry Mirror的命令如下：</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/%E7%8E%A9%E8%BD%ACdocker%E9%95%9C%E5%83%8F/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E5%AE%89%E5%85%A8%E7%AF%87/">
                                          
                                          <img src="/img/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E5%AE%89%E5%85%A8%E7%AF%87/">Google Kubernetes设计文档之安全篇</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        
                                        <p class="date-comments">
                                            <a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E5%AE%89%E5%85%A8%E7%AF%87/"><i class="far fa-calendar"></i> 2014-12-11</a>
                                        </p>
                                        
                                    </div>
                                    <p class="intro">摘要：Kubernetes是Google开源的容器集群管理系统，构建于Docker之上，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等功能。本文为其设计文档系列的第一篇：安全。
1.设计目标 本文讲述了Kubernetes的容器、API和基础设施在安全方面的设计原则。
 保证容器与其运行的宿主机之间有明确的隔离； 限制容器对基础设施或者其它容器造成不良影响的能力； 最小特权原则——限定每个组件只被赋予了执行操作所必需的最小特权，由此确保可能产生的损失达到最小； 通过清晰地划分组件的边界来减少需要加固和加以保护的系统组件数量。  2.设计要点 将etcd中的数据与minion节点和基础设施进行隔离 在Kubernetes的设计中，如果攻击者可以访问etcd中的数据，那么他就可以在宿主机上运行任意容器，获得存储在volumes或者pods的任何受保护信息（比如访问口令或者作为环境变量的共享密钥），通过中间人攻击来拦截和重定向运行中的服务流量，或者直接删除整个集群的历史信息。 **Kubernetes设计的基本原则是，对etcd中数据的访问权限应该只赋予某些特定的组件，这些组件或者需要对系统有完整的控制权，或者对系统服务变更请求能执行正确的授权和身份验证操作。**将来，etcd会提供粒度访问控制，但这样的粒度要求有一个管理员能够深刻理解etcd中存储数据的schema，并按照schema设置相应的安全策略。管理员必须能够在策略层面上保证Kubernetes的安全性，而非实现层面；另外，随着时间推移，数据的schema可能产生变化，这样的状况应该被预先考虑以免造成意外的安全泄漏。 Kubelet和Kube Proxy都需要与它们特定角色相关的信息——对于Kubelet，需要的是运行的pods集合的信息；对于Kube Proxy，需要用以负载均衡的服务与端点集合信息。Kubelet同样需要提供运行的pods和历史终止数据的相关信息。Kubelet和Kube Proxy用于加载配置的方式是“wait for changes”的HTTP请求。因此，限制Kubelet和Kube Proxy的权限使其只能访问对应角色所需的信息是可行的。 **Replication controller和其他future controller的controller manager经过用户授权可以代表其执行对Kubernetes资源的自动化维护。Controller manager访问或修改资源状态的权限应该被严格地限定在它们特定的职责范围之内，而不能访问其他无关角色的信息。**例如，一个replication controller只需要如下权限：创建已知pods配置的副本，设定已经存在的pods的运行状态，或者删除它创建的已存在的pods；而不需要知道pods的内容或者当前状态，亦不需要有访问挂载了volume的pods中的数据的权限。 Kubernetes pod scheduler负责从pod中读取数据并将其注入pod所在集群的minion节点中。它需要的最低限度的权限有，查看pod ID（用以生成binding）、pod当前状态、分配给pod的资源信息等。Pod scheduler不需要修改pods或查看其它资源的权限，只需要创建binding的权限。Pod scheduler不需要删除binding的权限，除非它接管了宕机机器上原有组件的重定位工作。在这样的情况下，scheduler可能需要对用户或者项目容器信息的读取权限来决定重定位pod的优先位置。
原文链接：Security in Kubernetes（编译/何思玫 审校/孙宏亮） [simple-author-box]</p>
                                    <p class="read-more"><a href="https://fengfees.github.io/blog/google-kubernetes%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%E4%B9%8B%E5%AE%89%E5%85%A8%E7%AF%87/" class="btn btn-template-main"></a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        

                        <ul class="pager">
                            
                            <li class="previous"><a href="/blog/">&larr; </a></li>
                            

                            
                            <li class="next"><a href="/blog/page/3/"> &rarr;</a></li>
                            
                        </ul>
                    </div>
                    

                    

                    

                    <div class="col-md-3">

                        

                        

<div class="panel panel-default sidebar-menu">

    <div class="panel-heading">
      <h3 class="panel-title"></h3>
    </div>

    <div class="panel-body">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" role="search">
            <div class="input-group">
                <input type="search" name="q" class="form-control" placeholder="">
                <input type="hidden" name="sitesearch" value="https://fengfees.github.io/">
                <span class="input-group-btn">
                    <button type="submit" class="btn btn-template-main"><i class="fas fa-search"></i></button>
                </span>
            </div>
        </form>
    </div>
</div>














<div class="panel sidebar-menu">

    <div class="panel-heading">
        <h3 class="panel-title"></h3>
    </div>

    <div class="panel-body">
        <ul class="tag-cloud">
            
            
            <li>
                <a href="/tags/bosh"><i class="fas fa-tags"></i> bosh</a>
            </li>
            
            <li>
                <a href="/tags/cf-release"><i class="fas fa-tags"></i> cf-release</a>
            </li>
            
            <li>
                <a href="/tags/cgroups"><i class="fas fa-tags"></i> cgroups</a>
            </li>
            
            <li>
                <a href="/tags/cloudfoundry"><i class="fas fa-tags"></i> cloudfoundry</a>
            </li>
            
            <li>
                <a href="/tags/collector"><i class="fas fa-tags"></i> collector</a>
            </li>
            
            <li>
                <a href="/tags/dea"><i class="fas fa-tags"></i> dea</a>
            </li>
            
            <li>
                <a href="/tags/docker"><i class="fas fa-tags"></i> docker</a>
            </li>
            
            <li>
                <a href="/tags/docker-network"><i class="fas fa-tags"></i> docker-network</a>
            </li>
            
            <li>
                <a href="/tags/etcd"><i class="fas fa-tags"></i> etcd</a>
            </li>
            
            <li>
                <a href="/tags/gorouter"><i class="fas fa-tags"></i> gorouter</a>
            </li>
            
            <li>
                <a href="/tags/haproxy"><i class="fas fa-tags"></i> haproxy</a>
            </li>
            
            <li>
                <a href="/tags/kubernetes"><i class="fas fa-tags"></i> kubernetes</a>
            </li>
            
            <li>
                <a href="/tags/libcontainer"><i class="fas fa-tags"></i> libcontainer</a>
            </li>
            
            <li>
                <a href="/tags/namespace"><i class="fas fa-tags"></i> namespace</a>
            </li>
            
            <li>
                <a href="/tags/news"><i class="fas fa-tags"></i> news</a>
            </li>
            
            <li>
                <a href="/tags/pipework"><i class="fas fa-tags"></i> pipework</a>
            </li>
            
            <li>
                <a href="/tags/runc"><i class="fas fa-tags"></i> runc</a>
            </li>
            
            <li>
                <a href="/tags/runtime"><i class="fas fa-tags"></i> runtime</a>
            </li>
            
            <li>
                <a href="/tags/security"><i class="fas fa-tags"></i> security</a>
            </li>
            
            <li>
                <a href="/tags/syslog_aggregator"><i class="fas fa-tags"></i> syslog_aggregator</a>
            </li>
            
        </ul>
    </div>

</div>






                        

                    </div>
                    

                    

                </div>
                
            </div>
            
        </div>
        

        <footer id="footer">
    <div class="container">

        
        <div class="col-md-4 col-sm-6">
            <h4></h4>

            <p><strong>关于我们：<strong> <p>浙大SEL实验室</p><p>地址：杭州市浙大路38号曹光彪西楼405室</p><p><strong></strong>
            <a href="/contact" class="btn btn-small btn-template-main" >跳转到关于</a>

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

        <div class="col-md-4 col-sm-6">

             
            <h4></h4>

            <div class="blog-entries">
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://fengfees.github.io/blog/docker%E5%AE%B9%E5%99%A8%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BA%91%E7%AC%AC2%E7%89%88%E6%8E%A8%E8%8D%90/">
                          
                            <img src="/img/banner.jpg" class="img-responsive" alt="《Docker容器与容器云》第2版推荐">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://fengfees.github.io/blog/docker%E5%AE%B9%E5%99%A8%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BA%91%E7%AC%AC2%E7%89%88%E6%8E%A8%E8%8D%90/">《Docker容器与容器云》第2版推荐</a></h5>
                    </div>
                </div>
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://fengfees.github.io/blog/docker%E5%AE%B9%E5%99%A8%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BA%91%E6%8E%A8%E8%8D%90/">
                          
                            <img src="/img/placeholder.png" class="img-responsive" alt="《Docker容器与容器云》推荐">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://fengfees.github.io/blog/docker%E5%AE%B9%E5%99%A8%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BA%91%E6%8E%A8%E8%8D%90/">《Docker容器与容器云》推荐</a></h5>
                    </div>
                </div>
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://fengfees.github.io/blog/docker%E8%83%8C%E5%90%8E%E7%9A%84%E6%A0%87%E5%87%86%E5%8C%96%E5%AE%B9%E5%99%A8%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E-runc/">
                          
                            <img src="/img/placeholder.png" class="img-responsive" alt="Docker背后的标准化容器执行引擎——runC">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://fengfees.github.io/blog/docker%E8%83%8C%E5%90%8E%E7%9A%84%E6%A0%87%E5%87%86%E5%8C%96%E5%AE%B9%E5%99%A8%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E-runc/">Docker背后的标准化容器执行引擎——runC</a></h5>
                    </div>
                </div>
                
            </div>

            <hr class="hidden-md hidden-lg">
             

        </div>
        

        
        <div class="col-md-4 col-sm-6">

          <h4></h4>

            <p><strong>联系我们: 
                                    <p>团队负责人：丁轶群 yiqunding@zju.edu.cn</p>
                                    <p>网站维护人：冯志凌 502361391@qq.com</p>                
    </strong>

            

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

    </div>
    
</footer>







<div id="copyright">
    <div class="container">
        <div class="col-md-12">
            
            <p class="pull-left">Copyright ©️ 2020 SEL Laboratory , ZJ University all rights reserved.</p>
            
            <p class="pull-right">
               <a href="https://bootstrapious.com/p/universal-business-e-commerce-template">Bootstrapious</a>.
              

               <a href="https://github.com/devcows/hugo-universal-theme">DevCows</a>.
            </p>
        </div>
    </div>
</div>





    </div>
    

    
<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>

<script src="/js/front.js"></script>


<script src="/js/owl.carousel.min.js"></script>



  </body>
</html>
